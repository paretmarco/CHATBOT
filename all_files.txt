File: chunk_to_smooth.py
import json
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

config_file_path = "config.json"

with open(config_file_path, 'r') as config_file:
    config = json.load(config_file)

input_file_path = config["input_file"]
output_file_path = "book_created/combined_output.json"

chunks = []
chapter_counter = 1

def is_title(line):
    return line.startswith("#") and line.endswith("#\n") and len(line.strip()) > 1

with open(input_file_path, 'r') as input_file:
    line_buffer = []

    for line in input_file:
        if is_title(line) and line_buffer:
            chunks.append(" ".join(line_buffer))
            line_buffer = []

        if is_title(line):
            chunks.append(f"#TITLE{chapter_counter}")
            chapter_counter += 1
            chunks.append(line.strip())
        else:
            line_buffer.append(line.strip())

    if line_buffer:
        chunks.append(" ".join(line_buffer))

with open(output_file_path, 'w') as output_file:
    json.dump(chunks, output_file)

logging.info(f'JSON file created: {output_file_path}')

with open(config_file_path, 'r') as config_file:
    config_data = json.load(config_file)

config_data["json_file"] = output_file_path

with open(config_file_path, 'w') as config_file:
    json.dump(config_data, config_file)

logging.info(f'Config file updated: {config_file_path}')

File: query_processor_smooth.py
import json
import logging
import os
from chatbot import process_chatbot_request

# Configure logging settings
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Load input chunks from a JSON file
def load_input_chunks(input_file):
    with open(input_file, "r") as infile:
        return json.load(infile)

# Write output chunks in a .txt file
def write_output_chunk(chunk, response, output_file):
    with open(output_file, "a") as outfile:
        outfile.write(response)
        outfile.write("\n\n")

# Load configuration from config.json
with open('config.json', 'r') as config_file:
    config = json.load(config_file)

input_file = config["json_file"]
output_file = config["input_file"]
additional_context = config["additional_context"]

# Load chunks from input file
chunks = load_input_chunks(input_file)

last_chunk = None
for chunk in chunks:
    if chunk.startswith("#") or chunk.startswith("TITLE") or not chunk.strip():  # Skip writing TITLE and treat blank records like records starting with "#".
        if not chunk.startswith("#TITLE"):
            write_output_chunk(chunk, chunk, output_file)
    else:
        if last_chunk and last_chunk.startswith("#") and not last_chunk.strip():
            user_input = f'Continua "{last_chunk}" utilizzando questo testo (###{chunk}###) ed aggiungendo spunti interessanti'
        else:
            user_input = f"Parafrasa e scrivi con lo stile di Victor Hugo: ###{chunk}### aggiungi spunti interessanti e se ci sono esercizi scrivili in fondo per punti."
        
        # Use the author_personality variable from the config
        author_personality = config['author_personality']
        max_tokens = config['max_tokens']

        # Send the request to the chatbot and get the response
        response = process_chatbot_request(user_input, max_tokens, author_personality, additional_context)
        write_output_chunk(chunk, response, output_file)
    
    last_chunk = chunk

# Log completion of the task
logging.info(f"Task completed: Paraphrased chunks are saved in '{output_file}'")

File: chatbot.py
import os
import openai
import requests
import json
from flask import Flask, request, jsonify
import logging
import re
from flask_cors import CORS

logging.basicConfig(level=logging.INFO)

app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": "*"}})

# Load default values from config.json
with open("config.json") as config_file:
    config = json.load(config_file)

default_max_tokens = config["max_tokens"]
default_author_personality = config["author_personality"]

# per sentenze complete
def is_complete_sentence(sentence):
    # Check if the sentence ends with a punctuation mark
    return bool(re.search(r'[.!?]\s*$', sentence))

# Set up OpenAI API
openai.api_key = os.environ["OPENAI_API_KEY"]

def search_snippets(query):
    # Log the query
    logging.info(f"Searching snippets for query: {query}")

    search_url = os.environ.get("SEARCH_URL", "http://127.0.0.1:5000/api/search")

    response = requests.post(search_url, json={"query": query})

    logging.info(f"Search response status code: {response.status_code}")

    if response.status_code == 200:
        return json.loads(response.text)
    else:
        return None

def process_chatbot_request(user_input, max_tokens, user_personality, additional_context):
    # Log the user input
    logging.info(f"Processing chatbot request with user_input: {user_input}")

    search_results = search_snippets(user_input)

    logging.info(f"Search results received: {search_results}")

    if search_results and search_results["response"]:
        context = search_results["response"][0]["response"]
    else:
        context = "I couldn't find any relevant information."

    # Update the user message to include the query, the words "to answer use", and the retrieved snippets
    user_message_content = f"{user_input}. To answer, use: {context}"

    if not is_complete_sentence(user_input):
        user_message_content = f"Please complete the following sentence: {user_input}. To answer, consider: {context}"

    # Calculate the remaining tokens available for the messages
    available_tokens = 4097 - max_tokens - 100  # reserve some tokens for the response

    # Truncate the user_message_content and context if necessary
    user_message_content = user_message_content[:available_tokens // 2]
    context = context[:available_tokens // 2]

    # Create the message sequence
    messages = [
        {"role": "system", "content": user_personality},
        {"role": "assistant", "content": additional_context + " " + context},
        {"role": "user", "content": user_message_content + " please use and adapt this material for your answer "}
    ]

    # Log messages sent to OpenAI API
    logging.info(f"Messages sent to OpenAI API: {messages}")

    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=messages,
        max_tokens=int(max_tokens),
        n=1,
        stop=None,
        temperature=0.8,
    )

    assistant_reply = response.choices[0].message['content']

    # Save the conversation
    with open("prev_conversations.json", "r+") as f:
        conversations = json.load(f)["conversations"]
        conversations.append({
            "role": "user",
            "content": user_input
        })
        conversations.append({
            "role": "assistant",
            "content": assistant_reply
        })
        f.seek(0)
        json.dump({"conversations": conversations}, f)
        f.truncate()

    return assistant_reply

@app.route("/api/chatbot", methods=["POST"])
def chatbot():
    try:
        data = request.json
        user_input = data["user_input"]
        max_tokens = data.get("max_tokens", default_max_tokens)
        user_personality = data.get("user_personality", default_author_personality)
        additional_context = data["additional_context"]

        logging.info(f"Received chatbot API request with user_input: {user_input}")

        response_text = process_chatbot_request(user_input, max_tokens, user_personality, additional_context)

        logging.info(f"Generated chatbot response: {response_text}")

        return jsonify({"response": response_text})
    except Exception as e:
        logging.error(f"Error in chatbot API: {str(e)}")

        return jsonify({"error": str(e)})

@app.route("/prev_conversations", methods=["GET"])
def prev_conversations():
    tokens_to_load = int(request.args.get("tokens", 2000))
    tokens_loaded = 0
    loaded_conversations = []

    with open("prev_conversations.json", "r") as f:
        conversations = json.load(f)["conversations"]

    for conversation in reversed(conversations):
        content_length = len(conversation["content"])
        if tokens_loaded + content_length > tokens_to_load:
            break
        loaded_conversations.insert(0, conversation)
        tokens_loaded += content_length

    return jsonify({"conversations": loaded_conversations})

if __name__ == "__main__":
    host = os.environ.get("FLASK_HOST", "127.0.0.1")  # Add this line to get the host value
    app.run(host=host, port=5001)
