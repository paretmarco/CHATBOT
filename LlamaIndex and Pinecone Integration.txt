Pinecone Quickstart

Installation: Install the Pinecone client for Python with the command pip install pinecone-client.

API key: Retrieve your API key from the Pinecone console. You can verify its validity by initializing Pinecone with the pinecone.init() method.

Index Creation: An index can be created with the pinecone.create_index() function. The example given creates an index named "quickstart" for 8-dimensional vectors with the Euclidean distance metric.

Index Retrieval: You can fetch a list of your indexes with pinecone.list_indexes().

Data Insertion: The upsert() method of the Index object is used to insert data into the index.

Index Statistics: Use describe_index_stats() method to fetch index details like dimension, fullness, and vector count.

Query Index: Query your index to fetch vectors most similar to an example vector using the query() method.

LlamaIndex and Pinecone Integration Manual
1. Prerequisites
Make sure to install required packages:
bash
Copy code
pip install llama_index
pip install pinecone
2. Updating to LlamaIndex 6.0
Update LlamaIndex using the command below:
bash
Copy code
pip install --upgrade llama_index
3. Querying Data From Pinecone With LlamaIndex
A. Basic LlamaIndex Querying
python
Copy code
from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader

# Load your data
documents = SimpleDirectoryReader('data').load_data()

# Build an index over the documents
index = GPTVectorStoreIndex.from_documents(documents)

# Query the index
query_engine = index.as_query_engine()
response = query_engine.query("What did the author do growing up?")
print(response)
B. Basic Pinecone Setup
python
Copy code
import os
import pinecone

# Initialize Pinecone
api_key = os.environ['PINECONE_API_KEY']
pinecone.init(api_key=api_key, environment="eu-west1-gcp")

# Create a Pinecone index if it doesn't exist
try {
    pinecone.create_index("quickstart-index", dimension=1536, metric="euclidean", pod_type="p1")
} except Exception:
    # Index already exists
    pass
pinecone_index = pinecone.Index("quickstart-index")
C. Integrating LlamaIndex and Pinecone
Follow these steps to use Pinecone as a vector store with LlamaIndex:

Initialize Pinecone:
python
Copy code
import pinecone
api_key = "your_api_key"
pinecone.init(api_key=api_key, environment="us-west1-gcp")
Create a Pinecone Index:
python
Copy code
pinecone.create_index("quickstart", dimension=1536, metric="euclidean", pod_type="p1")
pinecone_index = pinecone.Index("quickstart")
Load Documents:
python
Copy code
from llama_index import SimpleDirectoryReader
documents = SimpleDirectoryReader('./data').load_data()
Create a PineconeVectorStore:
python
Copy code
from llama_index.vector_stores import PineconeVectorStore
vector_store = PineconeVectorStore(pinecone_index=pinecone_index)
Create a GPTVectorStoreIndex:
python
Copy code
from llama_index.storage.storage_context import StorageContext
storage_context = StorageContext.from_defaults(vector_store=vector_store)
index = GPTVectorStoreIndex.from_documents(documents, storage_context=storage_context)
D. Filtering Results
python
Copy code
from llama_index.vector_stores.types import ExactMatchFilter, MetadataFilters

# Define metadata filters
filters = MetadataFilters(filters=[ExactMatchFilter(key='theme', value='Mafia')])

# Retrieve with filters
retriever = index.as_retriever(filters=filters)
retriever.retrieve('What is inception about?')

# Using Pinecone-specific keyword arguments
retriever = index.as_retriever(vector_store_kwargs={"filter": {"theme": "Mafia"}})
retriever.retrieve('What is inception about?')
E. Building a Bot
python
Copy code
from llama_index import SimpleDirectoryReader, GPTVectorStoreIndex, LLMPredictor, PromptHelper, StorageContext, load_index_from_storage
from langchain.chat_models import ChatOpenAI 

def index_documents(folder):
    prompt_helper = PromptHelper(max_input_size=4096, num_outputs=512, max_chunk_overlap=20, chunk_size_limit=600)
    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name="gpt-3.5-turbo", max_tokens=512))
    documents = SimpleDirectoryReader(folder).load_data()
    index = GPTVectorStoreIndex.from_documents(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper)
    index.storage_context.persist(persist_dir=".") # save in current directory
    return index

def my_chatGPT_bot(input_text):
    # load the index from vector_store.json
    storage_context = StorageContext.from_defaults(persist_dir=".")
    index = load_index_from_storage(storage_context)
    # create a query engine to ask question
    query_engine = index.as_query_engine()
    response = query_engine.query(input_text)
    return response.response

index_documents("training documents")
response = my_chatGPT_bot('Where is Singapore located?')
print(response)
F. Advanced Setup
python
Copy code
from langchain.agents import Tool, ToolConfig
from langchain.chains.conversation.memory import ConversationBufferMemory
from langchain.chat_models import ChatOpenAI
from langchain.agents import initialize_agent
from llama_index.langchain_helpers.agents import LlamaToolkit, create_llama_chat_agent, IndexToolConfig
from llama_index import LLMPredictor, PromptHelper, OpenAIEmbedding, ServiceContext
from langchain.llms import OpenAI

# Set up the Tools and Langchain Chatbot Agent
tool_config = IndexToolConfig(
    query_engine=query_engine, 
    name=f"Vector Index",
    description=f"useful for when you want to answer queries about X",
    tool_kwargs={{"return_direct": True}}
)

tool = LlamaIndexTool.from_tool_config(tool_config)

toolkit = LlamaToolkit(
    index_configs=index_configs,
)

agent_chain = create_llama_chat_agent(
    toolkit,
    llm,
    memory=memory,
    verbose=True
)

agent_chain.run(input="Query about X")
Don't forget to replace "your_api_key" with your actual Pinecone API key.
This manual provides examples of how to query data using LlamaIndex with Pinecone, create a bot, filter results, and set up an advanced chatbot. The examples are self-contained and provide a solid foundation for querying data in complex ways using Pinecone and LlamaIndex. The code can be easily modified to suit different use cases.

Note: For more details and examples, you can refer to the following resources:

Pinecone Vector Store Example (https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/PineconeIndexDemo.html)
Vector Store Integrations (https://gpt-index.readthedocs.io/en/latest/how_to/integrations/vector_stores.html)